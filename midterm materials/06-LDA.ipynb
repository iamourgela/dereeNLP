{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.3.12)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in /home/codespace/.local/lib/python3.12/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->kagglehub) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: WordCloud in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from WordCloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in /home/codespace/.local/lib/python3.12/site-packages (from WordCloud) (11.1.0)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (from WordCloud) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->WordCloud) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->WordCloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->WordCloud) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->WordCloud) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->WordCloud) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->WordCloud) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->WordCloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->WordCloud) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub \n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install WordCloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File `'/workspaces/dereeNLP/midterm'` not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/IPython/core/magics/execution.py:727\u001b[39m, in \u001b[36mExecutionMagics.run\u001b[39m\u001b[34m(self, parameter_s, runner, file_finder)\u001b[39m\n\u001b[32m    726\u001b[39m     fpath = arg_lst[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m727\u001b[39m     filename = \u001b[43mfile_finder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/IPython/utils/path.py:90\u001b[39m, in \u001b[36mget_py_filename\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m py_name\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFile `\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m` not found.\u001b[39m\u001b[33m\"\u001b[39m % name)\n",
      "\u001b[31mOSError\u001b[39m: File `'/workspaces/dereeNLP/midterm'` not found.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m     response.raise_for_status()\n\u001b[32m     10\u001b[39m     local_path.write_bytes(response.content)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrun\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43mstr(local_path)}\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2481\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2479\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2480\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2481\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2483\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2484\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2485\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2486\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/IPython/core/magics/execution.py:738\u001b[39m, in \u001b[36mExecutionMagics.run\u001b[39m\u001b[34m(self, parameter_s, runner, file_finder)\u001b[39m\n\u001b[32m    736\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.name == \u001b[33m'\u001b[39m\u001b[33mnt\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m re.match(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m^\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.*\u001b[39m\u001b[33m'\u001b[39m\u001b[33m$\u001b[39m\u001b[33m\"\u001b[39m,fpath):\n\u001b[32m    737\u001b[39m         warn(\u001b[33m'\u001b[39m\u001b[33mFor Windows, use double quotes to wrap a filename: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33mun \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmypath\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mmyfile.py\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    740\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fpath \u001b[38;5;129;01min\u001b[39;00m sys.meta_path:\n",
      "\u001b[31mException\u001b[39m: File `'/workspaces/dereeNLP/midterm'` not found."
     ]
    }
   ],
   "source": [
    "# This code is downloading the notebook from GitHub and running it\n",
    "import requests\n",
    "from pathlib import Path\n",
    "url = \"https://raw.githubusercontent.com/nbakas/NLP/refs/heads/main/02-Preprocessing.ipynb\"\n",
    "filename = url.split(\"/\")[-1]\n",
    "local_path = Path.cwd() / filename\n",
    "if not local_path.exists():\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    local_path.write_bytes(response.content)\n",
    "%run {str(local_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               good quality dog food\n",
       "1                       not advertise\n",
       "2                         delight say\n",
       "3                      cough medicine\n",
       "4                         great taffy\n",
       "                     ...             \n",
       "568449                    not without\n",
       "568450                   disappointed\n",
       "568451               perfect maltipoo\n",
       "568452    favorite train reward treat\n",
       "568453                    great honey\n",
       "Name: Summary, Length: 568454, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use gensim library for topic modeling\n",
    "# Import corpora module for document processing\n",
    "from gensim import corpora\n",
    "# Import LdaMulticore for parallel LDA implementation\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert df_summary to list of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good quality dog food',\n",
       " 'not advertise',\n",
       " 'delight say',\n",
       " 'cough medicine',\n",
       " 'great taffy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_texts = df_summary.astype(str).tolist()\n",
    "my_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['good', 'quality', 'dog', 'food'],\n",
       " ['not', 'advertise'],\n",
       " ['delight', 'say'],\n",
       " ['cough', 'medicine'],\n",
       " ['great', 'taffy']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_texts = [my_text.split() for my_text in my_texts]\n",
    "processed_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dictionary mapping words to their IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x7fe06cac07a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell is creating a dictionary of words and their IDs from the processed texts.\n",
    "my_dictionary = corpora.Dictionary(processed_texts)\n",
    "my_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 random items from the dictionary:\n",
      "Word ID 981: horrid\n",
      "Word ID 44994: sleepies\n",
      "Word ID 2632: term\n",
      "Word ID 34082: zotz!!!\n",
      "Word ID 29204: veja\n",
      "Word ID 54632: soya\n",
      "Word ID 55579: from...louisiana?!\n",
      "Word ID 51525: sobroso!\n",
      "Word ID 28974: tubs!\n",
      "Word ID 11312: product...no\n"
     ]
    }
   ],
   "source": [
    "# Print 10 random items from the dictionary to understand its structure\n",
    "print(\"10 random items from the dictionary:\")\n",
    "import random\n",
    "random_ids = random.sample(list(my_dictionary.keys()), 10)\n",
    "for word_id in random_ids:\n",
    "    print(f\"Word ID {word_id}: {my_dictionary[word_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57360"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x7fe06cac07a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out extreme values (optional) to improve LDA performance and quality\n",
    "# no_below=100: Remove words that appear in fewer than 100 documents (rare terms)\n",
    "#   - Removes noise and very specific terms that don't help identify general topics\n",
    "# no_above=0.1: Remove words that appear in more than 10% of documents (too common)\n",
    "#   - Removes overly common words that appear across many topics and don't help differentiate\n",
    "# This filtering reduces my_dictionary size, speeds up computation, and helps LDA focus on meaningful topic-specific words\n",
    "my_dictionary.filter_extremes(no_below=10, no_above=0.1)\n",
    "my_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmy_dictionary\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'my_dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "len(my_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is creating a \"bag-of-words\" representation of your processed texts using the Gensim library.\n",
    "# In the following line, `my_corpus = [my_dictionary.doc2bow(text) for text in processed_texts]`, each document in `processed_texts` is converted to a bag-of-words format using `doc2bow()`.\n",
    "my_corpus = [my_dictionary.doc2bow(text) for text in processed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpus[:10]\n",
    "# Each tuple (word_id, frequency) represents a word by its dictionary ID and how many times it appears in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 documents in the corpus, their words and their IDs\n",
    "for doc_id, doc in enumerate(my_corpus[:10]):\n",
    "    print(f\"Document {doc_id+1}:\")\n",
    "    for word_id, freq in doc:\n",
    "        word = my_dictionary[word_id]\n",
    "        print(f\"  Word ID {word_id} ('{word}'): Frequency {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note on Test Set Evaluation**\n",
    "\n",
    "Since **LDA is an unsupervised learning method**, we often use the full dataset to build the model — there are no labels to predict, as in supervised learning.\n",
    "\n",
    "However, using a **test set** can still be very useful.\n",
    "It helps:\n",
    "\n",
    "* Evaluate how well the model **generalizes to new, unseen data**\n",
    "* Compare models with different **numbers of topics**\n",
    "* Avoid **overfitting** to the training data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set LDA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10  # Number of topics to be extracted\n",
    "my_passes = 10 # Number of my_passes of the corpus through the model during training. More my_passes means better accuracy but longer runtime\n",
    "workers = 4  # Number of worker processes for parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will take ~10 minutes to train the model if the dictionary is not filtered.\n",
    "# https://radimrehurek.com/gensim/models/ldamulticore.html\n",
    "lda_model = LdaMulticore(\n",
    "    corpus=my_corpus, # The document-term list we created earlier\n",
    "    id2word=my_dictionary, # Maps word IDs to actual words for interpretable output\n",
    "    num_topics=num_topics, # Number of topics to extract \n",
    "    passes=my_passes, # Number of training my_passes through the corpus \n",
    "    workers=workers, # Number of parallel processes to use \n",
    "    alpha='symmetric', # Topic distribution prior - 'symmetric' gives equal probability to all topics initially\n",
    "    eta='auto' # Word distribution prior (influences how words are distributed across topics). 'auto' lets the model learn optimal word weights. β in notes.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate LDA model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A coherence score tells us if the words in a topic appear in similar texts or contexts.\n",
    "- For example, in a good topic like: [\"dog\", \"cat\", \"pet\", \"animal\"], these words often show up in the same documents.\n",
    "- It takes values between 0 and 1, with 1 being the highest coherence. Typical values are between 0.3 and 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/coherencemodel.html\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, # LDA model\n",
    "                                     texts=processed_texts, # list of texts, each text is a list of words\n",
    "                                     dictionary=my_dictionary, # dictionary of words and their IDs\n",
    "                                     coherence='c_v', # coherence measure, c_v id defined as the average pointwise mutual information of all word pairs in a topic\n",
    "                                     topn=20 # number of words to consider for coherence score\n",
    "                                     )\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity** is a measure of how well a topic model can **explain the words** in a new document.\n",
    "\n",
    "* It uses the **words in the document** to guess which topics are present.\n",
    "* Then it checks how **likely those words** are, based on the **learned topic-word probabilities**.\n",
    "* It does **not care about word order** — only which words appear and how often.\n",
    "\n",
    "**Lower perplexity = better fit**\n",
    "(The model is less “surprised” by the document’s words.)\n",
    "\n",
    "Even if the document has **multiple topics**, the perplexity can still be low — as long as the topics match well with the document’s words.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perplexity is the exponential of the negative average log-likelihood per word\n",
    "- Typical perplexity values for LDA models are usually in the range of 100–1000\n",
    "- Lower values (e.g., < 100) indicate better generalization (less surprise),\n",
    "- However, very low perplexity on the training set (e.g., < 50) can be a sign of overfitting,\n",
    "meaning the model fits the training data too closely and may not generalize well to unseen data\n",
    "- Very high values (e.g., > 1000) suggest poor topic modeling or an inappropriate number of topics\n",
    "  \n",
    "**Perplexity Formula:**\n",
    "\n",
    "If `log_perplexity` is the negative average log-likelihood per word (from Gensim):\n",
    "\n",
    "Perplexity = e^(-log_perplexity)\n",
    "\n",
    "Where:\n",
    "- `log_perplexity` is returned by `lda_model.log_perplexity(corpus)`\n",
    "- `exp` is the exponential function (base *e*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| log_perplexity | Actual perplexity | Interpretation                  |\n",
    "|----------------|-------------------|---------------------------------|\n",
    "| -5             | ~148              | Very good fit                   |\n",
    "| -6             | ~403              | Good                            |\n",
    "| -7             | ~1097             | Acceptable to borderline high   |\n",
    "| -8             | ~2980             | Likely too high → poor generalization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = lda_model.log_perplexity(my_corpus)\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize LDA topics using pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Prepare the visualization\n",
    "vis_data = gensimvis.prepare(lda_model, my_corpus, my_dictionary)\n",
    "\n",
    "# Set the figure size for better visualization\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Display the interactive visualization\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10 random documents and print their topics\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Select 10 random document indices\n",
    "random_doc_indices = random.sample(range(len(my_corpus)), 10)\n",
    "\n",
    "print(\"\\nTopic Distribution for 10 Random Documents:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for idx in random_doc_indices:\n",
    "    # Get the document's topic distribution\n",
    "    doc_topics = lda_model.get_document_topics(my_corpus[idx])\n",
    "    \n",
    "    # Sort topics by probability (highest first)\n",
    "    doc_topics = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the original text (if available)\n",
    "    original_text = df_summary.iloc[idx]\n",
    "    \n",
    "    print(f\"\\nDocument {idx}: \\\"{original_text}\\\"\")\n",
    "    print(\"Topic Distribution:\")\n",
    "    \n",
    "    for topic_id, prob in doc_topics[:3]:\n",
    "        # Get the top words for this topic\n",
    "        topic_words = lda_model.show_topic(topic_id, topn=5)\n",
    "        words = \", \".join([word for word, _ in topic_words])\n",
    "        \n",
    "        # Format the probability as a percentage\n",
    "        prob_percent = prob * 100\n",
    "        \n",
    "        print(f\"  Topic {topic_id+1}: {prob_percent:.2f}% ({words})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
